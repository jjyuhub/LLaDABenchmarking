# Workflow name
name: LLaDA Benchmarking and Profiling Workflow

# Trigger events
on:
  push:
    branches:
      - main
  workflow_dispatch:

# Jobs
jobs:
  benchmark_and_profile:
    name: Benchmark and Profile LLaDA Model
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      # Step 1: Clone LLaDA repository
      - name: Clone LLaDA Repository
        run: |
          echo "Cloning the LLaDA repository from GitHub..."
          git clone https://github.com/ML-GSAI/LLaDA
          cd LLaDA
          echo "Repository cloned successfully into directory: $(pwd)"

      # Step 2: Set up Python 3.8 (fix caching)
      - name: Set Up Python 3.8 Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
          # Specify the path to requirements.txt in the cloned repo
          cache: 'pip'
          cache-dependency-path: 'LLaDA/requirements.txt'  # Point to the cloned repo's file

      # Step 3: Install system dependencies
      - name: Install System Dependencies
        run: |
          echo "Installing system-level dependencies..."
          sudo apt-get update
          sudo apt-get install -y build-essential
          echo "System dependencies installed successfully."

      # Step 4: Install Python dependencies
      - name: Install Python Dependencies
        run: |
          echo "Installing Python dependencies..."
          pip install --upgrade pip
          if [ -f "LLaDA/requirements.txt" ]; then
            echo "Found requirements.txt, installing dependencies from it..."
            pip install -r LLaDA/requirements.txt
          else
            echo "No requirements.txt found, installing minimal dependencies..."
            pip install torch transformers==4.38.2
          fi
          echo "Dependencies installed successfully."

      # Step 5: Verify installed packages
      - name: Verify Installed Packages
        run: |
          echo "Verifying Python version..."
          python --version
          echo "Verifying pip version..."
          pip --version
          echo "Listing installed packages..."
          pip list

      # Step 6: Benchmark
      - name: Execute Benchmarking
        run: |
          echo "Starting benchmarking process..."
          python -c "
          import time
          from transformers import AutoModelForCausalLM, AutoTokenizer

          print('Loading model and tokenizer...')
          model = AutoModelForCausalLM.from_pretrained('GSAI-ML/LLaDA-8B-Base', trust_remote_code=True, torch_dtype='bfloat16')
          tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Base', trust_remote_code=True)

          prompt = 'Write a Python function to sort a list.'
          inputs = tokenizer(prompt, return_tensors='pt')

          print('Running inference...')
          start_time = time.time()
          outputs = model.generate(**inputs, max_length=50)
          end_time = time.time()

          generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
          num_tokens = len(tokenizer.encode(generated_text))
          elapsed_time = end_time - start_time
          tokens_per_sec = num_tokens / elapsed_time if elapsed_time > 0 else 0

          print(f'Generated text: {generated_text}')
          print(f'Number of tokens: {num_tokens}')
          print(f'Time taken: {elapsed_time:.2f} seconds')
          print(f'Tokens per second: {tokens_per_sec:.2f}')

          with open('benchmark_results.txt', 'w') as f:
              f.write(f'Generated text: {generated_text}\n')
              f.write(f'Number of tokens: {num_tokens}\n')
              f.write(f'Time taken: {elapsed_time:.2f} seconds\n')
              f.write(f'Tokens per second: {tokens_per_sec:.2f}\n')
          "
          echo "Benchmarking completed. Results saved to benchmark_results.txt."

      # Step 7: Upload benchmark results
      - name: Upload Benchmark Results Artifact
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results.txt
          retention-days: 7
        continue-on-error: true

      # Step 8: Profile
      - name: Execute Profiling
        run: |
          echo "Starting profiling process..."
          python -c "
          import torch
          from transformers import AutoModelForCausalLM, AutoTokenizer

          print('Loading model and tokenizer for profiling...')
          model = AutoModelForCausalLM.from_pretrained('GSAI-ML/LLaDA-8B-Base', trust_remote_code=True, torch_dtype='bfloat16')
          tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Base', trust_remote_code=True)

          prompt = 'Write a Python function to sort a list.'
          inputs = tokenizer(prompt, return_tensors='pt')

          print('Running profiling...')
          with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as prof:
              outputs = model.generate(**inputs, max_length=50)

          profiling_output = prof.key_averages().table(sort_by='cpu_time_total')
          print(profiling_output)
          with open('profiling_results.txt', 'w') as f:
              f.write(profiling_output)
          "
          echo "Profiling completed. Results saved to profiling_results.txt."

      # Step 9: Upload profiling results
      - name: Upload Profiling Results Artifact
        uses: actions/upload-artifact@v4
        with:
          name: profiling-results
          path: profiling_results.txt
          retention-days: 7
        continue-on-error: true

      # Step 10: Completion
      - name: Workflow Completion Notification
        run: |
          echo "Benchmarking and profiling workflow completed successfully!"
