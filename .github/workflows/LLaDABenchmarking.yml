name: LLaDA Benchmarking

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch: # Allows manual triggering

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y git python3 python3-pip

      - name: Clone LLaDA repository
        run: git clone https://github.com/ML-GSAI/LLaDA.git

      - name: Set up Python environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip

      - name: Install dependencies
        run: |
          source venv/bin/activate
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install transformers==4.38.2 numpy

      - name: Run Benchmarking
        run: |
          source venv/bin/activate
          python - <<EOF
          import time
          import torch
          from transformers import AutoModel, AutoTokenizer

          # Load model and tokenizer
          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"  # Fallback: "meta-llama/Llama-2-7b"
          try:
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
          except Exception as e:
              print("Model loading failed. Using fallback model.")
              tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")
              model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")

          model.eval()  # Ensure model is in evaluation mode

          # Prepare input
          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          # Measure inference time
          torch.cuda.synchronize() if torch.cuda.is_available() else None
          start_time = time.time()
          output_ids = model.generate(input_ids, max_length=128)
          torch.cuda.synchronize() if torch.cuda.is_available() else None
          end_time = time.time()

          # Token counting for accuracy
          num_generated_tokens = len(output_ids[0]) - len(input_ids[0])
          tokens_per_second = num_generated_tokens / (end_time - start_time)

          output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
          print(f"Generated Output:\n{output_text}\n")
          print(f"Tokens Generated: {num_generated_tokens}")
          print(f"Inference Speed: {tokens_per_second:.2f} tokens/sec")
          EOF

      - name: Profile Model Inference (CPU only)
        run: |
          source venv/bin/activate
          python - <<EOF
          import torch
          import torch.profiler
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
          model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)
          model.eval()

          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          with torch.profiler.profile(
              activities=[torch.profiler.ProfilerActivity.CPU],
              record_shapes=True
          ) as prof:
              _ = model.generate(input_ids, max_length=128)

          print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
          EOF
