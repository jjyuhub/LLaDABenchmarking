name: LLaDA Benchmarking (Windows GPU)

on:
  push:
    branches:
      - self-hosted
  pull_request:
    branches:
      - self-hosted
  workflow_dispatch:  # Allows manual triggering

jobs:
  benchmark:
    runs-on: [self-hosted, Windows, GPU]  # Runs on a local Windows runner with GPU

    steps:
      - name: Install system dependencies
        run: |
          if (-Not (Get-Command git -ErrorAction SilentlyContinue)) {
              Invoke-WebRequest -Uri "https://github.com/git-for-windows/git/releases/latest/download/Git-64-bit.exe" -OutFile "Git-Installer.exe"
              Start-Process -Wait -FilePath "Git-Installer.exe" -ArgumentList "/VERYSILENT"
              Remove-Item "Git-Installer.exe"
          }

          if (-Not (Get-Command python -ErrorAction SilentlyContinue)) {
              Invoke-WebRequest -Uri "https://www.python.org/ftp/python/3.10.11/python-3.10.11-amd64.exe" -OutFile "Python-Installer.exe"
              Start-Process -Wait -FilePath "Python-Installer.exe" -ArgumentList "/quiet InstallAllUsers=1 PrependPath=1"
              Remove-Item "Python-Installer.exe"
          }

      - name: Clone LLaDA repository
        run: |
          if (Test-Path "LLaDA") { Remove-Item -Recurse -Force "LLaDA" }
          git clone https://github.com/ML-GSAI/LLaDA.git

      - name: Set up Python environment
        run: |
          python -m venv venv
          venv\Scripts\activate
          python -m pip install --upgrade pip

      - name: Install dependencies
        run: |
          venv\Scripts\activate
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # CUDA 12.1
          pip install transformers==4.38.2 numpy

      - name: Create Python Benchmark Script
        run: |
          New-Item -Path . -Name "benchmark.py" -ItemType "file" -Force
          Set-Content -Path benchmark.py -Value @"
          import time
          import torch
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          FALLBACK_MODEL = "meta-llama/Llama-2-7b"

          device = 'cuda' if torch.cuda.is_available() else 'cpu'
          print(f"Running on device: {device}")

          try:
              print(f"Loading model: {MODEL_NAME}")
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main")
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main", torch_dtype=torch.bfloat16)
          except RuntimeError as e:
              print(f"Memory error: {e}\nFalling back to smaller model: {FALLBACK_MODEL}")
              tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)
              model = AutoModel.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)

          model.to(device)
          model.eval()

          # Define input
          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)

          # Ensure attention_mask is set to avoid unexpected behavior
          attention_mask = torch.ones_like(input_ids).to(device)  # Since padding is unknown, assume no padding

          # LLaDA-Specific: Use Custom Diffusion-Based Generation
          if "llada" in MODEL_NAME.lower():
              from LLaDA.generate import generate  # Import LLaDA's custom generation function

              start_time = time.time()
              output_ids = generate(model, input_ids, steps=128, gen_length=128, block_length=32, 
                                    temperature=0.0, cfg_scale=0.0, remasking='low_confidence')
              end_time = time.time()

              num_generated_tokens = output_ids.shape[1] - input_ids.shape[1]
          else:
              # Fallback to standard generate() for other models
              start_time = time.time()
              output_ids = model.generate(input_ids, max_length=128, attention_mask=attention_mask)
              end_time = time.time()

              num_generated_tokens = len(output_ids[0]) - len(input_ids[0])

          tokens_per_second = num_generated_tokens / (end_time - start_time)

          # Decode and print output
          output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
          print(f"Generated Output:\n{output_text}\n")
          print(f"Tokens Generated: {num_generated_tokens}")
          print(f"Inference Speed: {tokens_per_second:.2f} tokens/sec")
          "@

      - name: Run Benchmarking Script
        run: |
          venv\Scripts\activate
          python benchmark.py
