name: LLaDA Benchmarking

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:  # Allows manual triggering

jobs:
  benchmark:
    runs-on: ubuntu-latest  # Use ubuntu-20.04-xl if available for more RAM

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y git python3 python3-pip

      - name: Clone LLaDA repository
        run: git clone https://github.com/ML-GSAI/LLaDA.git

      - name: Set up Python environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip

      - name: Install dependencies
        run: |
          source venv/bin/activate
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install transformers==4.38.2 numpy

      - name: Run Benchmarking
        run: |
          source venv/bin/activate
          python - <<EOF
          import time
          import torch
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          FALLBACK_MODEL = "meta-llama/Llama-2-7b"

          try:
              print(f"Loading model: {MODEL_NAME}")
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main")
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main", torch_dtype=torch.bfloat16)
          except RuntimeError as e:
              print(f"Memory error: {e}\nFalling back to smaller model: {FALLBACK_MODEL}")
              tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)
              model = AutoModel.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)

          model.eval()

          # Define input
          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          # Ensure attention_mask is set to avoid unexpected behavior
          attention_mask = torch.ones_like(input_ids)  # Since padding is unknown, assume no padding

          # LLaDA-Specific: Use Custom Diffusion-Based Generation
          if "llada" in MODEL_NAME.lower():
              from LLaDA.generate import generate  # Import LLaDA's custom generation function

              start_time = time.time()
              output_ids = generate(model, input_ids, steps=128, gen_length=128, block_length=32, 
                                    temperature=0.0, cfg_scale=0.0, remasking='low_confidence')
              end_time = time.time()

              # Extract generated tokens beyond prompt length
              num_generated_tokens = output_ids.shape[1] - input_ids.shape[1]
          else:
              # Fallback to standard generate() for other models
              start_time = time.time()
              output_ids = model.generate(input_ids, max_length=128, attention_mask=attention_mask)
              end_time = time.time()

              num_generated_tokens = len(output_ids[0]) - len(input_ids[0])

          tokens_per_second = num_generated_tokens / (end_time - start_time)

          # Decode and print output
          output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
          print(f"Generated Output:\n{output_text}\n")
          print(f"Tokens Generated: {num_generated_tokens}")
          print(f"Inference Speed: {tokens_per_second:.2f} tokens/sec")
          EOF

      - name: Profile Model Inference (CPU only)
        run: |
          source venv/bin/activate
          python - <<EOF
          import torch
          import torch.profiler
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          FALLBACK_MODEL = "meta-llama/Llama-2-7b"

          try:
              print(f"Loading model for profiling: {MODEL_NAME}")
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main")
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main", torch_dtype=torch.bfloat16)
          except RuntimeError as e:
              print(f"Memory error: {e}\nFalling back to: {FALLBACK_MODEL}")
              tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)
              model = AutoModel.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)

          model.eval()

          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          with torch.profiler.profile(
              activities=[torch.profiler.ProfilerActivity.CPU],
              record_shapes=True
          ) as prof:
              if "llada" in MODEL_NAME.lower():
                  from LLaDA.generate import generate
                  _ = generate(model, input_ids, steps=128, gen_length=128, block_length=32)
              else:
                  _ = model.generate(input_ids, max_length=128)

          print("Profiling Output:")
          print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
          EOF
