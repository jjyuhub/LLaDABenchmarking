name: LLaDA Benchmarking (macOS)

on:
  push:
    branches:
      - macos-latest
  pull_request:
    branches:
      - macos-latest
  workflow_dispatch:  # Allows manual triggering

jobs:
  benchmark:
    runs-on: macos-latest  # Use the latest macOS runner

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew update
          brew install git python3

      - name: Clone LLaDA repository
        run: git clone https://github.com/ML-GSAI/LLaDA.git

      - name: Set up Python environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip

      - name: Install dependencies
        run: |
          source venv/bin/activate
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install transformers==4.38.2 numpy

      - name: Run Benchmarking
        run: |
          source venv/bin/activate
          python - <<EOF
          import time
          import torch
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          FALLBACK_MODEL = "meta-llama/Llama-2-7b"

          try:
              print(f"Loading model: {MODEL_NAME}")
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main")
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main", torch_dtype=torch.bfloat16)
          except RuntimeError as e:
              print(f"Memory error: {e}\nFalling back to smaller model: {FALLBACK_MODEL}")
              tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)
              model = AutoModel.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)

          model.eval()

          # Define input
          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          # Ensure attention_mask is set to avoid unexpected behavior
          attention_mask = torch.ones_like(input_ids)

          if "llada" in MODEL_NAME.lower():
              from LLaDA.generate import generate

              start_time = time.time()
              output_ids = generate(model, input_ids, steps=128, gen_length=128, block_length=32, 
                                    temperature=0.0, cfg_scale=0.0, remasking='low_confidence')
              end_time = time.time()

              num_generated_tokens = output_ids.shape[1] - input_ids.shape[1]
          else:
              start_time = time.time()
              output_ids = model.generate(input_ids, max_length=128, attention_mask=attention_mask)
              end_time = time.time()

              num_generated_tokens = len(output_ids[0]) - len(input_ids[0])

          tokens_per_second = num_generated_tokens / (end_time - start_time)
          output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
          print(f"Generated Output:\n{output_text}\n")
          print(f"Tokens Generated: {num_generated_tokens}")
          print(f"Inference Speed: {tokens_per_second:.2f} tokens/sec")
          EOF

      - name: Profile Model Inference (CPU only)
        run: |
          source venv/bin/activate
          python - <<EOF
          import torch
          import torch.profiler
          from transformers import AutoModel, AutoTokenizer

          MODEL_NAME = "GSAI-ML/LLaDA-8B-Base"
          FALLBACK_MODEL = "meta-llama/Llama-2-7b"

          try:
              print(f"Loading model for profiling: {MODEL_NAME}")
              tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main")
              model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True, force_download=False, revision="main", torch_dtype=torch.bfloat16)
          except RuntimeError as e:
              print(f"Memory error: {e}\nFalling back to: {FALLBACK_MODEL}")
              tokenizer = AutoTokenizer.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)
              model = AutoModel.from_pretrained(FALLBACK_MODEL, trust_remote_code=True)

          model.eval()

          prompt = "Write a Python function to sort a list."
          input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"]

          with torch.profiler.profile(
              activities=[torch.profiler.ProfilerActivity.CPU],
              record_shapes=True
          ) as prof:
              if "llada" in MODEL_NAME.lower():
                  from LLaDA.generate import generate
                  _ = generate(model, input_ids, steps=128, gen_length=128, block_length=32)
              else:
                  _ = model.generate(input_ids, max_length=128)

          print("Profiling Output:")
          print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
          EOF
